{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras keras-hub matplotlib --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def backend(line, cell):\n",
    "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
    "    if current == required:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        print(\n",
    "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
    "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa846c8a",
   "metadata": {},
   "source": [
    "## Chapter 3 - Introduction to TensorFlow, PyTorch, JAX, and Keras\n",
    "In this chapter, we will explore the most popular deep learning frameworks: TensorFlow, PyTorch, JAX, and Keras. We will discuss their features, advantages, and disadvantages, and provide examples of how to use each framework for building and training neural networks.\n",
    "What does a deep learning framework provide?\n",
    "1. **Tensor operations**: Efficient implementations of tensor operations for CPU (and optionally GPU).\n",
    "1. **Gradient computation**: Automatic differentiation to compute gradients for backpropagation.\n",
    "\n",
    "Those frameworks may also provide higher-level functionalities, such as:\n",
    "1. **Neural network layers**: Abstractions for common neural network layers (e.g., Dense, Convolutional).\n",
    "1. **Model abstractions**: High-level APIs for defining, training, and evaluating models.\n",
    "1. **Loss functions and optimizers**: Predefined loss functions and optimization algorithms.\n",
    "\n",
    "Keras, TensorFlow, PyTorch, and JAX do not have all the same features. Keras is a high-level API that can run on top of the other three frameworks, providing a user-friendly interface for building and training models. TensorFlow and PyTorch are more comprehensive frameworks that provide both low-level tensor operations and high-level model abstractions.\n",
    "\n",
    "### TensorFlow\n",
    "TensorFlow is an open-source deep learning framework developed by Google. It provides a comprehensive ecosystem for building and deploying machine learning models. TensorFlow supports both CPU and GPU computations and offers a wide range of tools and libraries for various machine learning tasks.\n",
    "Let's see some syntax examples in TensorFlow.\n",
    "\n",
    "#### Constant tensors\n",
    "Constant tensors are immutable tensors whose values cannot be changed after they are created. In TensorFlow, you can create constant tensors using the functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Useful for initializations\n",
    "t = tf.ones(shape=(2, 1))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = tf.zeros(shape=(2, 1))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = tf.constant([1, 2, 3], dtype=\"float32\")\n",
    "print(t, \"\\n\")\n",
    "\n",
    "\n",
    "# Random tensors\n",
    "# Normal distribution with mean and standard deviation\n",
    "t = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\n",
    "print(t, \"\\n\")\n",
    "\n",
    "\n",
    "# Uniform distribution with min and max values\n",
    "t = tf.random.uniform(shape=(3, 1))\n",
    "print(t, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d603c1e",
   "metadata": {},
   "source": [
    "#### Variable tensors\n",
    "Constant tensors are immutable, so they are not suitable for representing model parameters that need to be updated during training. For this purpose, TensorFlow provides variable tensors, which are mutable and can be updated using optimization algorithms. You can create variable tensors using the `tf.Variable` class. An initial value must be provided when creating a variable tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7976ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If uncommented, this code will result in an exception: the constant tensors are not assignable\n",
    "# t = tf.ones(shape=(1, 3), dtype=tf.dtypes.float32)\n",
    "# t[0, 1] = 0.0\n",
    "\n",
    "v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n",
    "print(v, \"\\n\")\n",
    "v.assign(tf.ones((3, 1)))\n",
    "print(v, \"\\n\")\n",
    "\n",
    "v[0, 0].assign(3.)\n",
    "print(v, \"\\n\")\n",
    "\n",
    "v.assign_add(tf.ones((3, 1)))\n",
    "print(v, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbb553",
   "metadata": {},
   "source": [
    "#### Tensor operations\n",
    "Here are some examples of basic tensor operations in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(shape=(2, 2), value=[2.0, 2.0, 2.0, 2.0])\n",
    "print(a, \"\\n\")\n",
    "b = tf.square(a)\n",
    "print(b, \"\\n\")\n",
    "c = tf.sqrt(a)\n",
    "print(c, \"\\n\")\n",
    "d = b + c\n",
    "print(d, \"\\n\")\n",
    "e = a @ b # Equivalent to tf.matmul(a, b)\n",
    "print(e, \"\\n\")\n",
    "f = tf.concat((a, b), axis=0)\n",
    "print(f, \"\\n\")\n",
    "g = tf.concat((a, b), axis=1)\n",
    "print(g, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8deca50",
   "metadata": {},
   "source": [
    "With these operations, we can implement the Dense layer from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f469f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, W, b):\n",
    "    return tf.nn.relu((inputs @ W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117ee43",
   "metadata": {},
   "source": [
    "#### Gradient computation\n",
    "The automatic differentiation feature in TensorFlow is exposed via the `tf.GradientTape` API. It allows you to record operations for automatic differentiation. It is necessary to open a `tf.GradientTape` context to record the operations for which you want to compute gradients. Beware that only operations executed within the context on variables watched by the `GradientTape` will be recorded for differentiation. By default, all `tf.Variable` objects are being watched. You can also manually watch tensors using the `watch` method of the `GradientTape` object. Here is an example of how to use `tf.GradientTape` to compute gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = tf.Variable(initial_value=3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    # function: y = x^2\n",
    "    result = tf.square(input_var)\n",
    "gradient = tape.gradient(result, input_var)\n",
    "# gradient (in this case derivative) of the function: y' = 2 * x\n",
    "# Evaluated in x = 3.0 --> y' = 2 * 3 = 6\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdef303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but with constants\n",
    "input_const = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_const) # required to compute the gradient w.r.t. constants\n",
    "    result = tf.square(input_const)\n",
    "gradient = tape.gradient(result, input_const)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41517ec5",
   "metadata": {},
   "source": [
    "It is possible to chain two GradientTape contexts to compute higher-order derivatives. Here is an example of how to compute the second derivative of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = tf.Variable([4.0])\n",
    "with tf.GradientTape() as second_order_tape:\n",
    "    with tf.GradientTape() as first_order_tape:\n",
    "        z = -(9.81 / 2) * time ** 2\n",
    "    z_dot = first_order_tape.gradient(z, time)\n",
    "z_ddot = second_order_tape.gradient(z_dot, time)\n",
    "print(\"Acceleration:\", z_ddot)\n",
    "print(\"Speed:\", z_dot)\n",
    "print(\"Position:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6548a6",
   "metadata": {},
   "source": [
    "#### Compilation with tf.function\n",
    "TensorFlow provides the `tf.function` decorator to compile Python functions into TensorFlow graphs. This can significantly improve performance by optimizing the execution of the function. It is also possible to compile with XLA (Accelerated Linear Algebra) to further optimize performance on supported hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442920d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input = np.ones(shape=(1, 3), dtype=\"float32\")\n",
    "W = tf.Variable(tf.ones(shape=(3, 1), dtype=tf.dtypes.float32))\n",
    "b = tf.Variable(tf.ones(shape=(3, 1), dtype=tf.dtypes.float32))\n",
    "\n",
    "@tf.function\n",
    "def dense(inputs, W, b):\n",
    "    return tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "print(dense(input, W, b), \"\\n\")\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def dense_jit(inputs, W, b):\n",
    "    return tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "print(dense_jit(input, W, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27a47b",
   "metadata": {},
   "source": [
    "As you saw in the previous examples, it is possible to send `np.ndarray` to a TensorFlow function, and it will be automatically converted to a TensorFlow tensor.\n",
    "#### Example: Linear regression with TensorFlow\n",
    "One example that can be done end-to-end with the current knowledge is a simple linear regression model using TensorFlow. We will classify two classes of points in 2D space using a linear model. Let's first create our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples_per_class = 1000\n",
    "# Using a 2D gaussian for the dataset\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[-2.5, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n",
    ")\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[2.5, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n",
    ")\n",
    "\n",
    "dataset = np.concat((negative_samples, positive_samples), axis=0, dtype=np.float32)\n",
    "print(\"Shape dataset:\", dataset.shape)\n",
    "\n",
    "labels = np.concat((np.zeros(shape=(1000,1)), np.ones(shape=(1000,1))), dtype=np.float32)\n",
    "print(\"Shape labels:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcafaa",
   "metadata": {},
   "source": [
    "We can use Matplotlib to visualize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=labels[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed198f5",
   "metadata": {},
   "source": [
    "Now let's implement a simple linear regression model using TensorFlow. We will define the model, loss function, and optimization algorithm, and then train the model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678925dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "training_rate = 1e-1\n",
    "\n",
    "W = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim), dtype=tf.dtypes.float32))\n",
    "b = tf.Variable(tf.zeros(shape=(output_dim, ), dtype=tf.dtypes.float32))\n",
    "\n",
    "def model(input, W, b):\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def lossFunction(output, label):\n",
    "    return tf.reduce_mean(tf.square(label - output))\n",
    "    \n",
    "# @tf.function(jit_compile=True)\n",
    "# @tf.function\n",
    "def batchTrainingStep(input_batch, labels, W, b):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input_batch, W, b)\n",
    "        loss = lossFunction(prediction, labels)\n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(training_rate * dW)\n",
    "    b.assign_sub(training_rate * db)\n",
    "    return loss\n",
    "    \n",
    "for i in range(40):\n",
    "    loss = batchTrainingStep(dataset, labels, W, b)\n",
    "    print(f\"Loss at step {i}: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32358b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(dataset, W, b)\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = -W[0] / W[1] * x + (0.5 - b) / W[1]\n",
    "plt.plot(x, y, \"-r\")\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=predictions[:, 0] > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253c708",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "PyTorch is an open-source deep learning framework developed by Facebook's AI Research lab. It has a first-class support for the famous model-sharing platform Hugging Face. It supports both CPU and GPU computations and offers a wide range of tools and libraries for various machine learning tasks. PyTorch API is higher level, including abstractions for layers, models and optimizers, making it easier to build and train neural networks.\n",
    "\n",
    "Let's see some syntax examples in PyTorch.\n",
    "\n",
    "#### Constant tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5efcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant tensors\n",
    "import torch\n",
    "\n",
    "# Useful for initializations\n",
    "t = torch.ones(size=(2, 1)) # In other platforms, the parameter it is called shape\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = torch.zeros(size=(2, 1))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(t, \"\\n\")\n",
    "\n",
    "# Random tensors\n",
    "t = torch.normal(\n",
    "        mean=torch.zeros(size=(3, 1)), # Mean and std deviation have to be of the same\n",
    "        std=torch.ones(size=(3, 1))    # size as the tensor itself\n",
    "    )\n",
    "print(t, \"\\n\")\n",
    "\n",
    "# Random tensor with uniform distribution\n",
    "t = torch.rand(3, 1) # Argument separated, different from TensorFlow and NumPy\n",
    "print(t, \"\\n\")\n",
    "\n",
    "# Another difference from TensorFlow: elements of tensors are assignable\n",
    "t[0, 0] = 1.\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5c3e9",
   "metadata": {},
   "source": [
    "Similar to `Variable`s in TensorFlow, PyTorch provides `Parameter`s that are tensors that can be optimized during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(size=(2, 1))\n",
    "p = torch.nn.parameter.Parameter(data=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b1d48",
   "metadata": {},
   "source": [
    "#### Tensor operations\n",
    "Here are some examples of basic tensor operations in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f15658",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[2.0, 2.0], [2.0, 2.0]], dtype=torch.float32)\n",
    "print(a, \"\\n\")\n",
    "b = torch.square(a)\n",
    "print(b, \"\\n\")\n",
    "c = torch.sqrt(a)\n",
    "print(c, \"\\n\")\n",
    "d = b + c\n",
    "print(d, \"\\n\")\n",
    "e = a @ b # Equivalent to torch.matmul(a, b)\n",
    "print(e, \"\\n\")\n",
    "f = torch.cat((a, b), dim=0)\n",
    "print(f, \"\\n\")\n",
    "g = torch.cat((a, b), dim=1)\n",
    "print(g, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65c806",
   "metadata": {},
   "source": [
    "And that's how a dense layer can be implemented from scratch in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, W, b):\n",
    "    return torch.relu(torch.matmul(inputs, W) + b)\n",
    "\n",
    "input = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "W = torch.tensor([[2.0], [2.0]], dtype=torch.float32)\n",
    "b = torch.tensor([[3.0]], dtype=torch.float32)\n",
    "\n",
    "print(dense(input, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e8c32",
   "metadata": {},
   "source": [
    "#### Gradient computation\n",
    "Behind the scenes, PyTorch uses a dynamic computation graph to compute gradients. This means that the graph is built on-the-fly while we perform operations on tensors. To enable gradient computation for a tensor, we need to set the `requires_grad` attribute to `True` when creating the tensor. PyTorch will then automatically track all operations performed on that tensor and build the computation graph accordingly. Here is an example of how to compute gradients in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = torch.tensor(3.0, requires_grad=True)\n",
    "result = torch.square(input_var)\n",
    "result.backward() # Performing back propagation on the output\n",
    "print(input_var.grad) # the 'grad' attribute gets filled\n",
    "\n",
    "result = torch.square(input_var)\n",
    "result.backward()\n",
    "print(input_var.grad) # Beware: it accumulates, it DOES NOT get overwritten\n",
    "\n",
    "input_var.grad = None # That's the way to reset the gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bf591",
   "metadata": {},
   "source": [
    "#### Example: Linear regression with PyTorch\n",
    "We can implement the same linear regression model using the PyTorch tools we introduced up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "W = torch.rand(input_dim, output_dim, requires_grad=True)\n",
    "b = torch.zeros(output_dim, requires_grad=True)\n",
    "\n",
    "def model(inputs, W, b):\n",
    "    return inputs @ W + b\n",
    "\n",
    "def mean_squared_error(targets, predictions):\n",
    "    per_sample_losses = torch.square(targets - predictions)\n",
    "    return torch.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "def training_step(inputs, targets, W, b):\n",
    "    predictions = model(inputs, W, b)\n",
    "    loss = mean_squared_error(targets, predictions)\n",
    "    loss.backward()\n",
    "    grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad\n",
    "    with torch.no_grad():\n",
    "        W -= grad_loss_wrt_W * learning_rate\n",
    "        b -= grad_loss_wrt_b * learning_rate\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    return loss\n",
    "\n",
    "dataset_torch = torch.tensor(dataset)\n",
    "labels_torch = torch.tensor(labels)\n",
    "for i in range(40):\n",
    "    loss = training_step(dataset_torch, labels_torch, W, b)\n",
    "    print(f\"Epoch: {i + 1}. Loss: {loss}\")\n",
    "    \n",
    "predictions = model(dataset_torch, W, b)\n",
    "W_plot = W.detach().numpy()\n",
    "b_plot = b.detach().numpy()\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = -W_plot[0] / W_plot[1] * x + (0.5 - b_plot) / W_plot[1]\n",
    "plt.plot(x, y, \"-r\")\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=predictions[:, 0] > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713a1cf",
   "metadata": {},
   "source": [
    "We implemented the linear regression model using only the low-level interface of PyTorch, without using any high-level abstractions.\n",
    "One of the high-level abstractions provided by PyTorch is the `Module` class, which can be used to define neural network layers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    # In the constructor, it is necessary to instantiate the 'Parameter's\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = torch.nn.Parameter(torch.rand(input_dim, output_dim))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    # It is required to override the 'forward' method.\n",
    "    def forward(self, inputs):\n",
    "        return torch.matmul(inputs, self.W) + self.b\n",
    "    \n",
    "# Let's call the model, the output will be random\n",
    "model = LinearModel()\n",
    "model(torch.tensor([2.0, 2.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb57e94",
   "metadata": {},
   "source": [
    "The other helpful high-level interface is the `torch.optim` module, which provides various optimization algorithms for training neural networks. We will use Stochastic Gradient Descent (SGD) to optimize our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def training_step(inputs, targets):\n",
    "    predictions = model(inputs)\n",
    "    loss = mean_squared_error(targets, predictions)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a787e",
   "metadata": {},
   "source": [
    "Finally, it is possible to use compilation in PyTorch with the decorator `torch.compile` (that can be used also as a function on a model) to optimize the execution of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def dense(inputs, W, b):\n",
    "    return torch.relu(torch.matmul(inputs, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4c211",
   "metadata": {},
   "source": [
    "However, the compilation is still experimental and rarely used in practice. In general, PyTorch is the slowest framework among the ones presented here.\n",
    "\n",
    "### JAX\n",
    "JAX is an open-source deep learning framework developed by Google. It is designed for high-performance numerical computing and provides a powerful automatic differentiation system. JAX supports CPU, GPU, and TPU and it is the fastest framework among the ones presented here.\n",
    "\n",
    "JAX encourages a functional programming style, where functions are pure and do not have side effects. This crucial aspect makes JAX code directly parallelizable, and so optimizable for GPU and TPU hardware.\n",
    "\n",
    "Let's see some syntax examples in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fea418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "# Useful for initializations\n",
    "t = jnp.ones(shape=(2, 1))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = jnp.zeros(shape=(2, 1))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "t = jnp.array([1, 2, 3], dtype=\"float32\")\n",
    "print(t, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d534054",
   "metadata": {},
   "source": [
    "JAX implements directly the same API as NumPy. However, there are few differences regarding array assignments and random number generation.\n",
    "\n",
    "#### Tensor assignments\n",
    "In JAX, arrays are immutable, meaning that once they are created, their values cannot be changed (it would go against the functional programming paradigm). Instead of modifying an array in place, you create a new array with the desired changes using several functions provided by JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = jnp.array([1, 2, 3], dtype=\"float32\")\n",
    "print(t, \"\\n\")\n",
    "new_t = t.at[0].set(10)\n",
    "print(new_t, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df5368",
   "metadata": {},
   "source": [
    "#### Pseudo-Random Number Generation\n",
    "Generating a random number using a Pseudo-Random Number Generator (PRNG), usually requires to hold a state. The state defines the current position of the PRNG in its sequence of random numbers. The stateless design of JAX requires to explicitly pass the state of the PRNG to the random number generation functions, and these functions return a new state along with the generated random numbers. This design choice allows for better reproducibility and easier parallelization of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "seed_key = jax.random.key(1337) # A seed is needed to generate a key, which will be used for the PRNGs\n",
    "t = jax.random.normal(seed_key, shape=(3,))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "# Given the stateless design, if we provide the same key, we will get the same value\n",
    "t = jax.random.normal(seed_key, shape=(3,))\n",
    "print(t, \"\\n\")\n",
    "\n",
    "# The function split provides a deterministic way to generate another key from a first one.\n",
    "# In this way, once we decided a first seed, we can generate more numbers in a deterministic way\n",
    "new_seed_key = jax.random.split(seed_key, num=1)[0]\n",
    "t = jax.random.normal(new_seed_key, shape=(3,))\n",
    "print(t, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd865be",
   "metadata": {},
   "source": [
    "#### Tensor operations\n",
    "In JAX, you can perform basic tensor operations using the same functions as in NumPy. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "a = 2.0 * jnp.ones((2, 2))\n",
    "print(a, \"\\n\")\n",
    "b = jnp.square(a)\n",
    "print(b, \"\\n\")\n",
    "c = jnp.sqrt(a)\n",
    "print(c, \"\\n\")\n",
    "d = b + c\n",
    "print(d, \"\\n\")\n",
    "e = a @ b # Equivalent to jnp.matmul(a, b)\n",
    "print(e, \"\\n\")\n",
    "e *= d # Element wise multiplication\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec7cf9",
   "metadata": {},
   "source": [
    "And that's how a dense layer can be implemented from scratch in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8278d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(inputs, W, b):\n",
    "    return jax.nn.relu(jnp.matmul(inputs, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395163bd",
   "metadata": {},
   "source": [
    "#### Gradient computation\n",
    "JAX provides the `jax.grad` function to compute gradients of functions. The `jax.grad` is a meta-function that takes a function as input and returns a new function that computes the gradient of the input function with respect to its arguments. Here is an example of how to use `jax.grad` to compute gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56425c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def compute_loss(input_var):\n",
    "    return jnp.square(input_var)\n",
    "\n",
    "# To get the gradient of a function, it should satisfy the following properties:\n",
    "#   1. It should return a scalar loss value (optionally, can have other outputs)\n",
    "#   2. Its first argument (that can be an array, list of array or dict of arrays),\n",
    "#      should contain the array we want to compute the gradient for \n",
    "grad_fn = jax.grad(compute_loss)\n",
    "\n",
    "input_var = jnp.array(3.0)\n",
    "grad_of_loss_wrt_input_var = grad_fn(input_var)\n",
    "print(grad_of_loss_wrt_input_var, \"\\n\")\n",
    "\n",
    "\n",
    "# Thanks to 'jax.value_and_grad', it is possible to get loss and gradient in one pass\n",
    "grad_fn = jax.value_and_grad(compute_loss)\n",
    "output, grad_of_loss_wrt_input_var = grad_fn(input_var)\n",
    "print(\"loss: \", output)\n",
    "print(\"grad: \", grad_of_loss_wrt_input_var, \"\\n\")\n",
    "\n",
    "# If the function of the gradient has more inputs and/or outputs, the gradient function\n",
    "# will also have that additional inputs and outputs\n",
    "def dense(state, inputs):\n",
    "    (W, b) = state\n",
    "    return jax.nn.relu(jnp.matmul(inputs, W) + b)[0], inputs\n",
    "\n",
    "grad_fn = jax.value_and_grad(dense, has_aux=True) # For auxiliary outputs, not needed when\n",
    "                                                  # more inputs are needed\n",
    "\n",
    "input = jnp.array([1.0, 0.0])\n",
    "W = jnp.array([1.0, 1.0])\n",
    "b = jnp.array([2.0])\n",
    "\n",
    "# Returns first the output of the differenciated function, then the gradient\n",
    "(loss, other), grad_of_loss_wrt_input_var = grad_fn((W, b), input)\n",
    "print(\"loss:\", loss)\n",
    "print(\"grad:\", grad_of_loss_wrt_input_var)\n",
    "print(\"inputs:\", other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950110bd",
   "metadata": {},
   "source": [
    "Finally, JAX also provides the `jax.jit` decorator to compile functions for improved performance. The `jax.jit` decorator compiles a function into a more efficient representation that can be executed faster on supported hardware (CPU, GPU, TPU). Here is an example of how to use `jax.jit` to compile a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def dense(inputs, W, b):\n",
    "    return jax.nn.relu(jnp.matmul(inputs, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9feeb",
   "metadata": {},
   "source": [
    "#### Example: Linear regression with JAX\n",
    "We can implement the same linear regression model using JAX. One important aspect to consider when using JAX is that it encourages a functional programming style. This means that we need to avoid side effects and mutable state in our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a762b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "def model(inputs, W, b):\n",
    "    return jnp.matmul(inputs, W) + b\n",
    "\n",
    "def mean_squared_error(targets, predictions):\n",
    "    per_sample_losses = jnp.square(targets - predictions)\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "def compute_loss(state, inputs, targets):\n",
    "    W, b = state\n",
    "    predictions = model(inputs, W, b)\n",
    "    loss = mean_squared_error(targets, predictions)\n",
    "    return loss\n",
    "\n",
    "grad_fn = jax.value_and_grad(compute_loss)\n",
    "\n",
    "@jax.jit\n",
    "def training_step(inputs, targets, W, b):\n",
    "    loss, grads = grad_fn((W, b), inputs, targets)\n",
    "    grad_wrt_W, grad_wrt_b = grads\n",
    "    W = W - grad_wrt_W * learning_rate # Beware! learning_rate must be constant!\n",
    "    b = b - grad_wrt_b * learning_rate\n",
    "    return loss, W, b\n",
    "\n",
    "W = jax.numpy.array(np.random.uniform(size=(input_dim, output_dim)))\n",
    "b = jax.numpy.array(np.zeros(shape=(output_dim,)))\n",
    "state = (W, b)\n",
    "for step in range(40):\n",
    "    loss, W, b = training_step(dataset, labels, W, b)\n",
    "    print(f\"Loss at step {step}: {loss:.4f}\")\n",
    "    \n",
    "predictions = model(dataset, W, b)\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = -W[0] / W[1] * x + (0.5 - b) / W[1]\n",
    "plt.plot(x, y, \"-r\")\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=predictions[:, 0] > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439478a",
   "metadata": {},
   "source": [
    "### Keras\n",
    "Keras is a high-level deep learning API that can run on top of TensorFlow, PyTorch, and JAX. It provides a user-friendly interface for building and training neural networks, making it easier to develop deep learning models without needing to deal with low-level details.\n",
    "\n",
    "Before using Keras, it is necessary to select the backend framework. By default, Keras uses TensorFlow as the backend, but it can be configured with an environment variable called `KERAS_BACKEND`. The supported backends are TensorFlow, PyTorch, and JAX. Alternatively, it is possible to set the backend in the configuration file located at `~/.keras/keras.json`. Here is an example of how to set the backend to PyTorch in the configuration file:\n",
    "\n",
    "```python\n",
    "{\n",
    "    # Default floating-point precision. It should typically not be\n",
    "    # changed.\n",
    "    \"floatx\": \"float32\",\n",
    "    # Default numerical fuzzing factor. It should typically not be\n",
    "    # changed.\n",
    "    \"epsilon\": 1e-07,\n",
    "    # Change \"tensorflow\" to \"jax\" or \"torch.\"\n",
    "    \"backend\": \"torch\",\n",
    "    # This is the default image layout. We'll talk about this in\n",
    "    # chapter 8.\n",
    "    \"image_data_format\": \"channels_last\",\n",
    "}\n",
    "```\n",
    "\n",
    "#### Layers in Keras\n",
    "Keras provides as central abstraction the `Layer` class, which can be used to define neural network layers. Keras provides many predefined layers, such as `Dense`, `Conv2D`, and `LSTM`. A layer is an object that stores some state (its parameters) and do some computation (the forward pass). As an example, here is how to create a class for a dense layer in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# All Keras layers inherit from the base Layer class.\n",
    "class SimpleDense(keras.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    # Weight creation takes place in the build() method.\n",
    "    def build(self, input_shape):\n",
    "        batch_dim, input_dim = input_shape\n",
    "        # add_weight is a shortcut method for creating weights. It's\n",
    "        # also possible to create standalone variables and assign them\n",
    "        # as layer attributes, like self.W = keras.Variable(shape=...,\n",
    "        # initializer=...).\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_dim, self.units), initializer=\"random_normal\"\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
    "\n",
    "    # We define the forward pass computation in the call() method.\n",
    "    def call(self, inputs):\n",
    "        y = keras.ops.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "my_dense = SimpleDense(units=32, activation=keras.ops.relu)\n",
    "input_tensor = keras.ops.ones(shape=(2, 784))\n",
    "\n",
    "# Call happens through the operator __call__(). We could ovveride this method, but then\n",
    "# we brake the internal automatic shape inference! The build function is exactly used for\n",
    "# this. When we run the __call__() function, the Layer will check if it has already been\n",
    "# built. If not, it will run the build method, that takes the shape as input. In the build\n",
    "# call, it is then possible to allocate the variable of the right size\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6b811",
   "metadata": {},
   "source": [
    "#### Models in Keras\n",
    "Keras also provides the `Model` class, which can be used to define neural network models as a graph of layers. A model is an object that groups layers into an object with training and inference features. Keras offers both a functional API and a object-oriented API for defining models. Keras' functional API will be discussed in further chapters. Here is an example of how to create a simple model using the `Sequential` subclass of `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04383579",
   "metadata": {},
   "source": [
    "The topology of the model is very important. It defines the hypothesis space of the model: choosing a model defines a specific space of functions that the model can represent, and with training, we are searching for the best function parameters within that space. It basically encodes our prior knowledge about the problem we want to solve. Selecting an appropriate model architecture is more an art than a science, and it often requires experimentation and domain knowledge.\n",
    "\n",
    "Once defined a model, it is possible to compile it using the `compile` method. This method configures the model for training by specifying the optimizer, loss function, and metrics to monitor during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "\n",
    "# These strings are used to instantiate the right objects\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# It is possible to provide directly the objects\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.1), # Using the object is useful for cases like this one\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654f3d1",
   "metadata": {},
   "source": [
    "After compiling the model, it is ready to be trained using the `fit` method, which takes the training data and labels as input and performs the training process.\n",
    "It requires as input: \n",
    "1. The training data.\n",
    "1. The training labels.\n",
    "1. The number of epochs (iterations over the entire dataset).\n",
    "1. The batch size (number of samples per gradient update).\n",
    "\n",
    "It is possible to also select part of the dataset for evaluation during training using the `validation_split` argument (or providing a validation dataset).\n",
    "\n",
    "The return type of the `fit` method is a `History` object, which contains information about the training process, such as the loss and metrics values for each epoch. This information can be useful for analyzing the model's performance and diagnosing potential issues during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e20f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Simple training without validation data\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    labels,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    ")\n",
    "pprint.pprint(history.history)\n",
    "\n",
    "# Training with validation data and custom learning rate\n",
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "indices_permutation = np.random.permutation(len(dataset))\n",
    "shuffled_inputs = dataset[indices_permutation]\n",
    "shuffled_targets = labels[indices_permutation]\n",
    "\n",
    "num_validation_samples = int(0.3 * len(dataset))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "history = model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets),\n",
    ")\n",
    "pprint.pprint(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13917aa9",
   "metadata": {},
   "source": [
    "Finally, the trained model can be used to make predictions on new data using the `predict` method. This method takes the input data as input and returns the predicted output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_inputs, batch_size=128)\n",
    "print(predictions[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
