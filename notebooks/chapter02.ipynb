{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Book at [deeplearningwithpython.io](https://deeplearningwithpython.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "%pip install keras keras-hub matplotlib --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def backend(line, cell):\n",
    "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
    "    if current == required:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        print(\n",
    "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
    "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Chapter 2 - The mathematical building blocks of neural networks\n",
    "### A first look at a neural network\n",
    "We here load the mnist dataset, which contains images of handwritten digits (0 to 9). Each image is 28 x 28 pixels, and each pixel is represented by a grayscale value between 0 and 255. The goal of the neural network is to classify these images into their respective digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now at the dataset we imported. It consists of 60,000 training samples and 10,000 test samples. Each sample is a 28x28 grayscale image, represented as a 2D array of pixel values ranging from 0 to 255. With ``shape``, we can see the dimensions of the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 28, 28)\n",
      "Test set shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", train_images.shape)\n",
    "print(\"Test set shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first axis (or dimension) always represents the samples, while all the other dimentions represent the data that will be provided to the neural network.\n",
    "Let's now see the labels for both the training a test set. The label are in the form of an array of digits between 0 and 9, with the i-th label being the digit represented in the i-th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training labels: 60000\n",
      "Training labels: [5 0 4 ... 5 6 8]\n",
      "Number of test labels: 10000\n",
      "Test labels: [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training labels:\", len(train_labels))\n",
    "print(\"Training labels:\", train_labels)\n",
    "\n",
    "print(\"Number of test labels:\", len(test_labels))\n",
    "print(\"Test labels:\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First neural network example! In this moment, not all the details have to be clear. The important point is to get a general idea of what a neural network is doing. We will go through all the details in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topology of the network has a first dense layer with 512 units and ReLU activation function, followed by a second dense layer with 10 units and softmax activation function. The first layer takes as input the 784-dimensional vector (28x28 pixels flattened) and outputs a 512-dimensional vector. The second layer takes this 512-dimensional vector and outputs a 10-dimensional vector, which represents the probabilities of each digit class (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer used is \"adam\", which is a popular optimization algorithm for training neural networks. The loss function used is \"sparse_categorical_crossentropy\", which is suitable for multi-class classification problems where the labels are provided as integers. The metric used to evaluate the model's performance during training and testing is \"accuracy\", which measures the proportion of correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ``fit`` method, we train the model for 5 epochs (iterations over the entire training dataset) with a batch size of 128 samples. The training process will output the loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9277 - loss: 0.2580\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9694 - loss: 0.1052\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9796 - loss: 0.0687\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9864 - loss: 0.0476\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e48bd6d7290>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the trained model to make predictions on the test set using the ``predict`` method. This will output an array of predicted probabilities for each class (0-9) for each test sample. We can then use ``argmax`` function to get the class with the highest probability for each sample, which gives us the predicted digit labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Prediction vector: [3.1338152e-06 5.9011097e-07 3.3277516e-05 1.0968582e-03 1.7676082e-09\n",
      " 8.5597021e-06 2.7874744e-10 9.9867558e-01 1.5543135e-05 1.6653792e-04]\n",
      "Prediction: 7\n",
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "test_digits = test_images[0:10]\n",
    "some_labels = test_labels[0:10]\n",
    "predictions = model.predict(test_digits)\n",
    "print(\"Prediction vector:\", predictions[0])\n",
    "print(\"Prediction:\", predictions[0].argmax())\n",
    "print(\"Label:\", some_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to evaluate if our model learned well, we use the ``evaluate`` method on the test set. This will output the loss and accuracy on the test data, which gives us an idea of how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.0649\n",
      "test_acc: 0.9789999723434448\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Data representations for neural networks\n",
    "The most common way to represent data for neural networks is using multi-dimensional arrays, also known as **tensors**. Tensors are generalizations of matrices to higher dimensions. For example, a 2D tensor is a matrix, a 3D tensor can be thought of as a stack of matrices (a cube of numbers) and so on. Tensors can have any number of dimensions, and each dimension is called an **axis**. The number of axes is called the **rank** of the tensor. For example, a scalar (single number) is a rank-0 tensor, a vector (1D array) is a rank-1 tensor, a matrix (2D array) is a rank-2 tensor, and so on.\n",
    "\n",
    "To get the rank of a tensor in NumPy, we can use the ``ndim`` attribute of a NumPy array. While, using ``shape`` attribute, we can get the dimensions of the tensor along each axis.\n",
    "\n",
    "#### Scalars (rank-0 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(\"x:\", x)\n",
    "print(\"Rank of x:\", x.ndim)\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, the rank of the scalar tensor is 0, and its shape is an empty tuple, indicating that it has no dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Vectors (rank-1 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x = np.array([12, 3, 6, 14, 7])\n",
    "print(\"x:\", x)\n",
    "print(\"Rank of x:\", x.ndim)\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Matrices (rank-2 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "print(\"x:\", x)\n",
    "print(\"Rank of x:\", x.ndim)\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Rank-3 tensors and higher-rank tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]]])\n",
    "print(\"x:\", x)\n",
    "print(\"Rank of x:\", x.ndim)\n",
    "print(\"Shape of x:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Key attributes\n",
    "Three are the key attributes of tensors that are important to understand when working with neural networks:\n",
    "- **Rank**: The number of axes (dimensions) of the tensor.\n",
    "- **Shape**: A tuple of integers representing the size of the tensor along each axis.\n",
    "- **Data type**: The type of data stored in the tensor (e.g., float32, int32, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(\"Rank of dataset:\", train_images.ndim)\n",
    "print(\"Shape of dataset:\", train_images.shape)\n",
    "print(\"Datatype of dataset:\", train_images.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-topic: show images from the dataset using matplotlib\n",
    "``matplotlib`` is a popular library for data visualization in Python. We can use it to display images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(\"Label:\", train_labels[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Slicing tensors in NumPy\n",
    "Slicing consists in selecting specific portions of a tensor. In NumPy, we can slice tensors using the colon (`:`) operator along with indices. The syntax for slicing is `start:stop:step`, where `start` is the index to start the slice (inclusive), `stop` is the index to end the slice (exclusive), and `step` is the step size for the slice. Default values for `start`, `stop`, and `step` are 0, the size of the axis (number of elements along the axis), and 1, respectively. We can slice along multiple axes by separating the slice specifications with commas or using multiple square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100]\n",
    "print(\"Slice shape:\", my_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, :, :]\n",
    "print(\"Equivalent slice shape:\", my_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, 0:28, 0:28]\n",
    "print(\"Another equivalent slice shape:\", my_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"Image sliced along the pixels dimensions. It slices the bottom-right part of all images.\")\n",
    "my_slice = train_images[:, 14:, 14:]\n",
    "print(\"Slice shape:\", my_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"Image sliced along the pixels dimensions. It slices the central part of all images.\")\n",
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "print(\"Slice shape:\", my_slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### The notion of data batches\n",
    "Usually, along the first axis of a tensor, we have the samples (or data points). When training neural networks, it is common to process the data in smaller groups called **batches**. Batching helps to reduce memory consumption and can lead to faster convergence during training.\n",
    "\n",
    "When we slice a tensor along the first axis, we are effectively selecting a batch of samples. For example, if we have a tensor of shape (60000, 28, 28) representing 60,000 images of size 28x28 pixels, slicing it as `tensor[0:128]` will give us a batch of the first 128 images, resulting in a new tensor of shape (128, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# First batch\n",
    "batch = train_images[:128]\n",
    "\n",
    "# Second batch\n",
    "batch = train_images[128:256]\n",
    "\n",
    "# Third batch, with generic implementation\n",
    "n = 3\n",
    "batch = train_images[128 * n : 128 * (n + 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Real-world examples of data tensors\n",
    "Real-world data can come in various forms, and neural networks can handle different types of data tensors. Here are some examples:\n",
    "##### Vector data\n",
    "When we have real-world data represented as vectors, each sample is a 1D array of features. For example, in a dataset of house prices, each sample could represent a house with features such as size, number of bedrooms, location, etc. If we have 1000 houses and each house has 10 features, the data tensor would have a shape of (1000, 10).\n",
    "##### Timeseries data or sequence data\n",
    "Timeseries data consists of sequences of data points collected over time. Each sample is a sequence of values, and the length of the sequence can vary. For example, in a dataset of stock prices, we collect bid and ask prices every minute. If we want to track an entire market-day session (6.5 hours), we would have 390 minutes of data per day. If we have data for 1000 days, the data tensor would have a shape of (1000, 390, 2), where 2 represents the bid and ask prices.\n",
    "##### Image data\n",
    "Image data is usually a 3D tensor, where each sample is a 2D array of pixel values with multiple channels (e.g., RGB channels for color images). For example, in the MNIST dataset, each image is a grayscale image of size 28x28 pixels. If we have 60,000 images, the data tensor would have a shape of (60000, 28, 28, 1), where 1 represents the single channel for grayscale images. For color images, each image would have 3 channels (Red, Green, Blue), resulting in a shape of (60000, 28, 28, 3).\n",
    "##### Video data\n",
    "Video data can be represented as a 4D tensor, where each sample is a sequence of frames (images, see above) over time. For example, if we have a dataset of 1000 videos, each video consisting of 30 frames of size 64x64 pixels with 3 color channels (RGB), the data tensor would have a shape of (1000, 30, 64, 64, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Tensor operations\n",
    "Of course, neural networks are just a set of parametrized (differentiable) functions applied to the input tensors. These functions are made by composing a set of basic tensor operations. In this section, we will explore some of the most common tensor operations used in neural networks.\n",
    "#### Element-wise operations\n",
    "The simplest tensor operations are element-wise operations, which are applied independently to each element of the tensor. They can be unary (involving a single tensor) or binary (involving two tensors). For eample, Rectified Linear Unit (ReLU) activation function is an element-wise unary operation that sets all negative values in the tensor to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    x = x.copy()\n",
    "    if len(x.shape) == 1:\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] = max(x[i], 0)\n",
    "    else:\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] = naive_relu(x[i])\n",
    "    return x\n",
    "\n",
    "array = np.array([[1, 2, -3], [2, -1, 3], [0, -32, -1]])\n",
    "print(\"Array:\\n\", array)\n",
    "naive_relu(array)\n",
    "print(\"\\nArray w/ ReLU:\\n\", array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple example is the addition of two tensors of the same shape, which is an element-wise binary operation that adds corresponding elements from both tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == len(y.shape)\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()\n",
    "    if len(x.shape) == 1:\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] += y[i]\n",
    "    else:\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] = naive_add(x[i], y[i])\n",
    "    return x\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "y = np.array([[10, 20, 30], [40, 50, 60]])\n",
    "print(\"\\ny:\\n\", y)\n",
    "\n",
    "result = naive_add(x, y)\n",
    "print(\"\\nx + y:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the real implementation of these operations is optimized and done in low-level languages like C or C++ for performance reasons. You can see the speed difference when working with large tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = x + y\n",
    "    z = np.maximum(z, 0.0)\n",
    "print(\"NumPy took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = naive_add(x, y)\n",
    "    z = naive_relu(z)\n",
    "print(\"Naive took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Element-wise operations can also be performed on tensors of different shapes, thanks to a mechanism called **broadcasting**. Broadcasting is a new tensor operation that expands the smaller tensor along the dimensions of the larger tensor so that they have compatible shapes for the operation. This can be done only when there is no ambiguity in the expansion process. For example, when adding a tensor of shape (32, 10) with a tensor of shape (10,), the smaller tensor is broadcasted to match the shape of the larger tensor, resulting in a new tensor of shape (32, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.random((32, 10))\n",
    "y = np.random.random((10,))\n",
    "\n",
    "y = np.expand_dims(y, axis=0)\n",
    "Y = np.tile(y, (32, 1))\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, when performing operations with broadcasting-compatible tensors, the smaller tensor is not actually copied in memory to match the shape of the larger tensor, it would be terribly inefficient. Instead, the operation is performed as if the smaller tensor was expanded, without actually creating a new tensor in memory. See the following example with ``np.maximum`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "z = np.maximum(x, y)\n",
    "print(\"Shape max:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor dot products\n",
    "Another important tensor operation is the **dot product** (or matrix multiplication) between two tensors. The dot product is a generalization of the dot product between vectors, as also simple 2D matrix multiplication can be seen as a dot product between rows and columns of the matrices. In NumPy, we can perform dot products using the ``np.dot`` function or the ``@`` operator.\n",
    "\n",
    "Remember, the dot product in general is not commutative, meaning that the order of the operands matters. For example, ``A @ B`` is not the same as ``B @ A``.\n",
    "\n",
    "The condition of compatibility for the dot product is that the inner dimensions of the two tensors must match.\n",
    "For example, when multiplying a tensor of shape (32, 10) with a tensor of shape (10, 64), the inner dimensions (10) match, and the resulting tensor will have a shape of (32, 64).\n",
    "This generalizes to higher-rank tensors as well. For example, when multiplying a tensor of shape (4, 3, 2) with a tensor of shape (2, 4, 5), the inner dimensions (2) match, and the resulting tensor will have a shape of (4, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor reshaping\n",
    "Sometimes, we need to change the shape of a tensor without changing its data. This is called **reshaping**. In NumPy, we can reshape tensors using the ``reshape`` method or the ``np.reshape`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(\"x:\\n\", x)\n",
    "print(\"Shape of x:\", x.shape)\n",
    "\n",
    "x = x.reshape((6, 1))\n",
    "print(\"\\nReshaped x:\\n\", x)\n",
    "print(\"Shape of x:\", x.shape)\n",
    "\n",
    "x = x.reshape((2, 3))\n",
    "print(\"\\nReshaped x:\\n\", x)\n",
    "print(\"Shape of x:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing a tensor means swapping its axes. In NumPy, we can transpose tensors using the ``T`` attribute or the ``np.transpose`` function. Transposing is particularly useful when we need to change the order of dimensions for operations like dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((300, 10, 50, 20))\n",
    "x = np.transpose(x)\n",
    "print(\"Shape of x transpose:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reimplementation of the first neural network example\n",
    "Let's reimplement the first neural network example replacing some utilities from Keras with more low-level operations. This will help us understand better what is happening under the hood when training a neural network.\n",
    "\n",
    "##### Dense layer implementation\n",
    "We start with the Dense layer implementation. A Dense layer is a fully connected layer where each neuron is connected to every neuron in the previous layer. The output of a Dense layer is computed as the dot product between the input tensor and the weight matrix, followed by the addition of a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        self.activation = activation\n",
    "        self.W = keras.Variable(\n",
    "            shape=(input_size, output_size), initializer=\"uniform\"\n",
    "        )\n",
    "        self.b = keras.Variable(shape=(output_size,), initializer=\"zeros\")\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = ops.matmul(inputs, self.W)\n",
    "        x = x + self.b\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequential model implementation\n",
    "Now, we can implement a simple Sequential model that stacks multiple Dense layers together. The Sequential model will store the layers and provide a method to perform the forward pass through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model\n",
    "Now, we can build the model by creating an instance of the Sequential class and adding Dense layers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential(\n",
    "    [\n",
    "        NaiveDense(input_size=28 * 28, output_size=512, activation=ops.relu),\n",
    "        NaiveDense(input_size=512, output_size=10, activation=ops.softmax),\n",
    "    ]\n",
    ")\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drawing batches from the dataset\n",
    "To train the model, we need to draw batches of data from the training dataset. We can implement a class that handles the batching process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        # No check needed for overflow, NumPy does it for us\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weight update\n",
    "Finally, we can implement the weight update process. A naive implementation would simply go in the direction of the negative gradient scaled by the learning rate. This is called Stochastic Gradient Descent (SGD). We can implement it or we can use the optimizer provided by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "    \n",
    "learning_rate = 1e-3\n",
    "def update_weights_manual(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign(w - g * learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient computation and training loop\n",
    "The last step is to compute the gradients and implement the training loop. We can use TensorFlow's automatic differentiation capabilities to compute the gradients of the loss with respect to the model's weights. Then, we can update the weights using the optimizer.\n",
    "\n",
    "In TensorFlow, we can use the `tf.GradientTape` context manager to record the operations for automatic differentiation. Inside the `GradientTape` context, we perform the forward pass and compute the loss. After exiting the context, we can use the `tape.gradient` method to compute the gradients of the loss with respect to the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%backend tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.zeros(shape=())\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x) # GradientTape watches only Variable objects. This watch forces GT to track operations involving x\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "\n",
    "print(\"Gradient:\", grad_of_y_wrt_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%backend tensorflow\n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        loss = ops.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = ops.mean(loss)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights_manual(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop implementation\n",
    "Finally, we can implement a simple training loop to train the model on the MNIST dataset. The training loop will perform the following steps for each epoch:\n",
    "1. Perform a forward pass through the model to compute the predictions.\n",
    "2. Compute the loss between the predictions and the true labels.\n",
    "3. Compute the gradients of the loss with respect to the model's weights.\n",
    "4. Update the model's weights using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%backend tensorflow\n",
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%backend tensorflow\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the trained model on the test dataset to see how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%backend tensorflow\n",
    "predictions = model(test_images)\n",
    "predicted_labels = ops.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "f\"accuracy: {ops.mean(matches):.2f}\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chapter02",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
