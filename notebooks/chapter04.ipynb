{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras keras-hub matplotlib --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f961a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def backend(line, cell):\n",
    "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
    "    if current == required:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        print(\n",
    "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
    "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0e732",
   "metadata": {},
   "source": [
    "## Chapter 4 - Classification and regression\n",
    "In this chapter, we will explore the fundamental concepts of classification and regression, two essential problems in supervised machine learning. We will discuss binary classification, multi-class classification, and at the end of the chapter, we will cover regression problems as well.\n",
    "\n",
    "### 4.1 Binary Classification - IMDb Movie Reviews\n",
    "Binary classification involves categorizing data into one of two classes. The example we are going to use is the sentiment analysis of IMDb movie reviews, where the goal is to classify reviews as either positive or negative. The reviews are pre-labeled and quite polarized, making it a suitable dataset for binary classification tasks. Let's start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Every review is a list of words taken from a dictionary. With 'num_words=10000',\n",
    "# we are limiting the number of words to include into this dictionary to 10000.\n",
    "# The dictionary is ordered with the first elements being the one more frequent.\n",
    "# That means that dropping after the first 10k words, we are dropping words that\n",
    "# probably were refered once or twice, so they are not very descriptive of the\n",
    "# 'sentiment' of the review.\n",
    "num_words = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=num_words\n",
    ")\n",
    "\n",
    "print(train_data[0][:10]) # First 10 words of the review\n",
    "print(train_labels[0]) # Labels are \\in {0, 1}, 0 is negative, 1 is positive\n",
    "\n",
    "# To decode a review, we can take the dictionary:\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Reverse it:\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# And finally decode a review:\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]\n",
    ")\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eabd9",
   "metadata": {},
   "source": [
    "Now, how can we build a model to classify these reviews? The input, as it is, is not easily transformable into a tensor. We need to preprocess the text data. Two common approaches are:\n",
    "1. Padding and truncating sequences to a fixed length. We then turn it into a tensor of shape (num_samples, max_length) of integers, where each integer represents a word index.\n",
    "2. Using one-hot encoding to represent each review as a binary vector of size equal to the vocabulary size. The vector will have 1s at the indices corresponding to the words present in the text and 0s elsewhere. Beware, with this approach we lose the concept of word order. The neural network will not be able to distinguish between \"The movie was great\" and \"Great was the movie\".\n",
    "\n",
    "We will use the second approach for this example. Here's how we can preprocess the data and build a simple neural network model using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multi_hot_encode(sequences, num_classes):\n",
    "    results = np.zeros((len(sequences), num_classes))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        # It takes the i-th element of results, and it sets\n",
    "        # to one the element whose index is in the sequence\n",
    "        results[i][sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "train_data_processed = multi_hot_encode(train_data, num_classes=num_words)\n",
    "test_data_processed = multi_hot_encode(test_data, num_classes=num_words)\n",
    "\n",
    "print(train_data_processed[0])\n",
    "\n",
    "train_labels = train_labels.astype(\"float32\")\n",
    "test_labels = test_labels.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0395e1b",
   "metadata": {},
   "source": [
    "Now we are ready to build and train our model. When designing a model for binary classification, the last layer should have a single unit with a sigmoid activation function (restricting the output to the range [0, 1], representing a probability). Every element of of the dataset is a 1D tensor, and on this class of problems Dense layers with relu activations work well.\n",
    "\n",
    "We still have to decide on the number of layers and units per layer. To guide a minimum the decision, we can imagine that every layer is learning a representation of the input data. Thanks to the non-linearity of the relu activation, we can learn complex representations. The more output units we have, the more complex the representation can be. However, having too many units can lead to overfitting, learning unwanted patterns in the training data that do not generalize well to unseen data.\n",
    "\n",
    "We will use two layers with 16 units each. The reason for this choice will be clear in next chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553e761",
   "metadata": {},
   "source": [
    "Now we have to compile the model. For binary classification, we typically use the **binary cross-entropy** loss function, which measures the difference between the predicted probability distribution and the ground truth distribution. The binary cross-entropy loss is defined as:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "$$\n",
    "\n",
    "What does this formula mean? We are computing the cross-entropy, that is a measure of dissimilarity between two probability distributions, between the true labels \\(y_i\\) and the predicted probabilities \\(p_i\\). If we use the same values for \\(y_i\\) and \\(p_i\\), for example the true labels, the loss function will represent the entropy of the distribution of the labels. The cross-entropy is always higher than or equal to the entropy, reaching equality when the two distributions are identical. Therefore, minimizing the cross-entropy loss will lead to better predictions. For more details on binary cross-entropy, refer to this [blog post](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a/).\n",
    "\n",
    "\n",
    "Back to the model compilation, we will use the Adam optimizer, which is virtually always a good choice. We will additionally monitor the accuracy metric during training.\n",
    "\n",
    "It is also important to set aside a validation set to monitor the model's performance on unseen data during training. We can do this by splitting the training data into a smaller training set and a validation set, or we can use the `validation_split` parameter in the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_processed,\n",
    "    train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2, # Alternative to validation_data=(val_data, val_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a8c93",
   "metadata": {},
   "source": [
    "We can check the what happened during training by plotting the training and validation loss and accuracy over epochs. We can use the history object returned by the `fit` method to access this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45543d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(\"History labels:\", history_dict.keys())\n",
    "\n",
    "loss = history_dict[\"loss\"]\n",
    "val_loss = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"r--\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"[IMDB] Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a498c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the figure\n",
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"r--\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"[IMDB] Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164833b1",
   "metadata": {},
   "source": [
    "In the plots, we can see that the training loss decreases over epochs, indicating that the model is learning from the training data. However, the validation loss starts to increase after a certain number of epochs, indicating that the model is **overfitting** to the training data. It means that the model is learning patterns that are specific to the training data, maybe due to spurious correlations, and does not generalize well to unseen data. We will see techniques to mitigate overfitting in the next chapters. Here we can see that after 4 epochs the model starts to overfit. Therefore, we can choose to stop training after 4 epochs to get the best performance on unseen data.\n",
    "\n",
    "Finally, we can evaluate the model on the test set to see how well it generalizes to unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb82f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(train_data_processed, train_labels, epochs=4, batch_size=512)\n",
    "results = model.evaluate(test_data_processed, test_labels)\n",
    "print(\"Result loss on test data:\", results[0])\n",
    "print(\"Result accuracy on test data:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186abe2d",
   "metadata": {},
   "source": [
    "Let's test the model with a new review written from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fffb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "review : str = \"I don't think I ever saw a movie like this one. It was extremely boring and long! I fell asleep around 13 times!\"\n",
    "indicized_review = [1] + [word_index.get(word, 0) + 3 for word in review.split()]\n",
    "\n",
    "# Let's see how the decoded review looks like after removal of out-of-vocabulary words\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in indicized_review]\n",
    ")\n",
    "print(decoded_review)\n",
    "\n",
    "tokenized_review = multi_hot_encode([indicized_review], num_words) # [indicized_reviews] because the method wants a 2D array\n",
    "prediction = model.predict(tokenized_review)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbd861",
   "metadata": {},
   "source": [
    "We can now test some changes. For example, we can try to increase the number of units in each layer or add more layers to see if the model's performance improves. However, we should be cautious about overfitting, especially with a small dataset. We can also experiment with different activation functions, optimizers, learning rates, and loss functions to see how they affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(4, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    x=train_data_processed,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "history_dict = history.history\n",
    "print(\"History labels:\", history_dict.keys())\n",
    "\n",
    "loss = history_dict[\"loss\"]\n",
    "val_loss = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss, \"r--\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"r--\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "results = model.evaluate(test_data_processed, test_labels)\n",
    "print(\"Result loss on test data:\", results[0])\n",
    "print(\"Result accuracy on test data:\", results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
