{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras keras-hub matplotlib --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f961a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def backend(line, cell):\n",
    "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
    "    if current == required:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        print(\n",
    "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
    "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0e732",
   "metadata": {},
   "source": [
    "## Chapter 4 - Classification and regression\n",
    "In this chapter, we will explore the fundamental concepts of classification and regression, two essential problems in supervised machine learning. We will discuss binary classification, multi-class classification, and at the end of the chapter, we will cover regression problems as well.\n",
    "\n",
    "### 4.1 Binary Classification - IMDb Movie Reviews\n",
    "Binary classification involves categorizing data into one of two classes. The example we are going to use is the sentiment analysis of IMDb movie reviews, where the goal is to classify reviews as either positive or negative. The reviews are pre-labeled and quite polarized, making it a suitable dataset for binary classification tasks. Let's start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Every review is a list of words taken from a dictionary. With 'num_words=10000',\n",
    "# we are limiting the number of words to include into this dictionary to 10000.\n",
    "# The dictionary is ordered with the first elements being the one more frequent.\n",
    "# That means that dropping after the first 10k words, we are dropping words that\n",
    "# probably were refered once or twice, so they are not very descriptive of the\n",
    "# 'sentiment' of the review.\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000\n",
    ")\n",
    "\n",
    "print(train_data[0][:10]) # First 10 words of the review\n",
    "print(train_labels[0]) # Labels are \\in {0, 1}, 0 is negative, 1 is positive\n",
    "\n",
    "# To decode a review, we can take the dictionary:\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Reverse it:\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# And finally decode a review:\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]\n",
    ")\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eabd9",
   "metadata": {},
   "source": [
    "Now, how can we build a model to classify these reviews? The input, as it is, is not easily transformable into a tensor. We need to preprocess the text data. Two common approaches are:\n",
    "1. Padding and truncating sequences to a fixed length. We then turn it into a tensor of shape (num_samples, max_length) of integers, where each integer represents a word index.\n",
    "2. Using one-hot encoding to represent each review as a binary vector of size equal to the vocabulary size. The vector will have 1s at the indices corresponding to the words present in the text and 0s elsewhere. Beware, with this approach we lose the concept of word order. The neural network will not be able to distinguish between \"The movie was great\" and \"Great was the movie\".\n",
    "\n",
    "We will use the second approach for this example. Here's how we can preprocess the data and build a simple neural network model using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multi_hot_encode(sequences, num_classes):\n",
    "    results = np.zeros((len(sequences), num_classes))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        # It takes the i-th element of results, and it sets\n",
    "        # to one the element whose index is in the sequence\n",
    "        results[i][sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "train_data_processed = multi_hot_encode(train_data, num_classes=10000)\n",
    "test_data_processed = multi_hot_encode(test_data, num_classes=10000)\n",
    "\n",
    "print(train_data_processed[0])\n",
    "\n",
    "train_labels = train_labels.astype(\"float32\")\n",
    "test_labels = test_labels.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0395e1b",
   "metadata": {},
   "source": [
    "Now we are ready to build and train our model. When designing a model for binary classification, the last layer should have a single unit with a sigmoid activation function (restricting the output to the range [0, 1], representing a probability). Every element of of the dataset is a 1D tensor, and on this class of problems Dense layers with relu activations work well.\n",
    "\n",
    "We still have to decide on the number of layers and units per layer. To guide a minimum the decision, we can imagine that every layer is learning a representation of the input data. Thanks to the non-linearity of the relu activation, we can learn complex representations. The more output units we have, the more complex the representation can be. However, having too many units can lead to overfitting, learning unwanted patterns in the training data that do not generalize well to unseen data.\n",
    "\n",
    "We will use two layers with 16 units each. The reason for this choice will be clear in next chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553e761",
   "metadata": {},
   "source": [
    "Now we have to compile the model. For binary classification, we typically use the **binary cross-entropy** loss function, which measures the difference between the predicted probability distribution and the ground truth distribution. The binary cross-entropy loss is defined as:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "$$\n",
    "\n",
    "What does this formula mean? We are computing the cross-entropy, that is a measure of dissimilarity between two probability distributions, between the true labels \\(y_i\\) and the predicted probabilities \\(p_i\\). If we use the same values for \\(y_i\\) and \\(p_i\\), for example the true labels, the loss function will represent the entropy of the distribution of the labels. The cross-entropy is always higher than or equal to the entropy, reaching equality when the two distributions are identical. Therefore, minimizing the cross-entropy loss will lead to better predictions. For more details on binary cross-entropy, refer to this [blog post](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a/).\n",
    "\n",
    "\n",
    "Back to the model compilation, we will use the Adam optimizer, which is virtually always a good choice. We will additionally monitor the accuracy metric during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_processed,\n",
    "    train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
